{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90f04f0e-183b-4666-a815-7cc590ce56ef",
   "metadata": {},
   "source": [
    "## Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fbc8152-6ebf-48c0-84d0-1d7e24c81f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45e9d437-d9b6-438f-b675-07fe123c180f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,precision_recall_fscore_support\n",
    "from sklearn.metrics import f1_score,roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec62974b-3c77-47d0-90d4-c9fd1246daf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Flow Duration</th>\n",
       "      <th>Total Fwd Packets</th>\n",
       "      <th>Total Backward Packets</th>\n",
       "      <th>Total Length of Fwd Packets</th>\n",
       "      <th>Total Length of Bwd Packets</th>\n",
       "      <th>Fwd Packet Length Max</th>\n",
       "      <th>Fwd Packet Length Min</th>\n",
       "      <th>Fwd Packet Length Mean</th>\n",
       "      <th>Fwd Packet Length Std</th>\n",
       "      <th>Bwd Packet Length Max</th>\n",
       "      <th>...</th>\n",
       "      <th>min_seg_size_forward</th>\n",
       "      <th>Active Mean</th>\n",
       "      <th>Active Std</th>\n",
       "      <th>Active Max</th>\n",
       "      <th>Active Min</th>\n",
       "      <th>Idle Mean</th>\n",
       "      <th>Idle Std</th>\n",
       "      <th>Idle Max</th>\n",
       "      <th>Idle Min</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>6</td>\n",
       "      <td>18.500000</td>\n",
       "      <td>17.677670</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>142377</td>\n",
       "      <td>46</td>\n",
       "      <td>62</td>\n",
       "      <td>1325</td>\n",
       "      <td>105855</td>\n",
       "      <td>570</td>\n",
       "      <td>0</td>\n",
       "      <td>28.804348</td>\n",
       "      <td>111.407285</td>\n",
       "      <td>4344</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>118873</td>\n",
       "      <td>23</td>\n",
       "      <td>28</td>\n",
       "      <td>1169</td>\n",
       "      <td>45025</td>\n",
       "      <td>570</td>\n",
       "      <td>0</td>\n",
       "      <td>50.826087</td>\n",
       "      <td>156.137367</td>\n",
       "      <td>2896</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>143577</td>\n",
       "      <td>43</td>\n",
       "      <td>55</td>\n",
       "      <td>1301</td>\n",
       "      <td>107289</td>\n",
       "      <td>570</td>\n",
       "      <td>0</td>\n",
       "      <td>30.255814</td>\n",
       "      <td>115.178969</td>\n",
       "      <td>4344</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>143745</td>\n",
       "      <td>49</td>\n",
       "      <td>59</td>\n",
       "      <td>1331</td>\n",
       "      <td>110185</td>\n",
       "      <td>570</td>\n",
       "      <td>0</td>\n",
       "      <td>27.163265</td>\n",
       "      <td>108.067176</td>\n",
       "      <td>4344</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56656</th>\n",
       "      <td>234</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>232</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>116</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56657</th>\n",
       "      <td>133288</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>94</td>\n",
       "      <td>482</td>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>241</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56658</th>\n",
       "      <td>11507694</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>450</td>\n",
       "      <td>3525</td>\n",
       "      <td>450</td>\n",
       "      <td>0</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>201.246118</td>\n",
       "      <td>3525</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>893.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>893</td>\n",
       "      <td>893</td>\n",
       "      <td>6503640.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6503640</td>\n",
       "      <td>6503640</td>\n",
       "      <td>DoS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56659</th>\n",
       "      <td>11507707</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>416</td>\n",
       "      <td>11632</td>\n",
       "      <td>416</td>\n",
       "      <td>0</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>147.078211</td>\n",
       "      <td>5792</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>897.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>897</td>\n",
       "      <td>897</td>\n",
       "      <td>6503122.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6503122</td>\n",
       "      <td>6503122</td>\n",
       "      <td>DoS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56660</th>\n",
       "      <td>11512204</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>326</td>\n",
       "      <td>11632</td>\n",
       "      <td>326</td>\n",
       "      <td>0</td>\n",
       "      <td>40.750000</td>\n",
       "      <td>115.258405</td>\n",
       "      <td>10184</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>892.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>892</td>\n",
       "      <td>892</td>\n",
       "      <td>6507197.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6507197</td>\n",
       "      <td>6507197</td>\n",
       "      <td>DoS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56661 rows Ã— 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Flow Duration  Total Fwd Packets  Total Backward Packets  \\\n",
       "0                  4                  2                       0   \n",
       "1             142377                 46                      62   \n",
       "2             118873                 23                      28   \n",
       "3             143577                 43                      55   \n",
       "4             143745                 49                      59   \n",
       "...              ...                ...                     ...   \n",
       "56656            234                  2                       2   \n",
       "56657         133288                  2                       2   \n",
       "56658       11507694                  5                       4   \n",
       "56659       11507707                  8                       6   \n",
       "56660       11512204                  8                       5   \n",
       "\n",
       "       Total Length of Fwd Packets  Total Length of Bwd Packets  \\\n",
       "0                               37                            0   \n",
       "1                             1325                       105855   \n",
       "2                             1169                        45025   \n",
       "3                             1301                       107289   \n",
       "4                             1331                       110185   \n",
       "...                            ...                          ...   \n",
       "56656                           64                          232   \n",
       "56657                           94                          482   \n",
       "56658                          450                         3525   \n",
       "56659                          416                        11632   \n",
       "56660                          326                        11632   \n",
       "\n",
       "       Fwd Packet Length Max  Fwd Packet Length Min  Fwd Packet Length Mean  \\\n",
       "0                         31                      6               18.500000   \n",
       "1                        570                      0               28.804348   \n",
       "2                        570                      0               50.826087   \n",
       "3                        570                      0               30.255814   \n",
       "4                        570                      0               27.163265   \n",
       "...                      ...                    ...                     ...   \n",
       "56656                     32                     32               32.000000   \n",
       "56657                     47                     47               47.000000   \n",
       "56658                    450                      0               90.000000   \n",
       "56659                    416                      0               52.000000   \n",
       "56660                    326                      0               40.750000   \n",
       "\n",
       "       Fwd Packet Length Std  Bwd Packet Length Max  ...  \\\n",
       "0                  17.677670                      0  ...   \n",
       "1                 111.407285                   4344  ...   \n",
       "2                 156.137367                   2896  ...   \n",
       "3                 115.178969                   4344  ...   \n",
       "4                 108.067176                   4344  ...   \n",
       "...                      ...                    ...  ...   \n",
       "56656               0.000000                    116  ...   \n",
       "56657               0.000000                    241  ...   \n",
       "56658             201.246118                   3525  ...   \n",
       "56659             147.078211                   5792  ...   \n",
       "56660             115.258405                  10184  ...   \n",
       "\n",
       "       min_seg_size_forward  Active Mean  Active Std  Active Max  Active Min  \\\n",
       "0                        20          0.0         0.0           0           0   \n",
       "1                        20          0.0         0.0           0           0   \n",
       "2                        32          0.0         0.0           0           0   \n",
       "3                        20          0.0         0.0           0           0   \n",
       "4                        20          0.0         0.0           0           0   \n",
       "...                     ...          ...         ...         ...         ...   \n",
       "56656                    32          0.0         0.0           0           0   \n",
       "56657                    32          0.0         0.0           0           0   \n",
       "56658                    32        893.0         0.0         893         893   \n",
       "56659                    32        897.0         0.0         897         897   \n",
       "56660                    32        892.0         0.0         892         892   \n",
       "\n",
       "       Idle Mean  Idle Std  Idle Max  Idle Min   Label  \n",
       "0            0.0       0.0         0         0  BENIGN  \n",
       "1            0.0       0.0         0         0  BENIGN  \n",
       "2            0.0       0.0         0         0  BENIGN  \n",
       "3            0.0       0.0         0         0  BENIGN  \n",
       "4            0.0       0.0         0         0  BENIGN  \n",
       "...          ...       ...       ...       ...     ...  \n",
       "56656        0.0       0.0         0         0  BENIGN  \n",
       "56657        0.0       0.0         0         0  BENIGN  \n",
       "56658  6503640.0       0.0   6503640   6503640     DoS  \n",
       "56659  6503122.0       0.0   6503122   6503122     DoS  \n",
       "56660  6507197.0       0.0   6507197   6507197     DoS  \n",
       "\n",
       "[56661 rows x 78 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read dataset\n",
    "df = pd.read_csv('./data/CICIDS2017_sample.csv') \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e119e353-5e74-4950-b8f8-8e0a237a606d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "BENIGN          22731\n",
       "DoS             19035\n",
       "PortScan         7946\n",
       "BruteForce       2767\n",
       "WebAttack        2180\n",
       "Bot              1966\n",
       "Infiltration       36\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec1e4a4-b8e3-486b-bf5d-835cc6f9d8ea",
   "metadata": {},
   "source": [
    "### Preprocessing (normalization and padding values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f387e72b-da70-4c98-99d7-c5b4b87a4dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-score normalization\n",
    "features = df.dtypes[df.dtypes != 'object'].index\n",
    "df[features] = df[features].apply(\n",
    "    lambda x: (x - x.mean()) / (x.std()))\n",
    "# Fill empty values by 0\n",
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "430151d9-a164-4d95-b9ea-4f628ce1cfca",
   "metadata": {},
   "source": [
    "EXAMPLE\n",
    "\n",
    "\n",
    "Original DataFrame:\r\n",
    "     A   B     C\r\n",
    "0  1.0  10   foo\r\n",
    "1  2.0  20   bar\r\n",
    "2  3.0  30   baz\r\n",
    "3  NaN  40   qux\r\n",
    "4  5.0  50  quux\r\n",
    "\r\n",
    "Normalized DataFrame:\r\n",
    "          A         B     C\r\n",
    "0 -1.183216 -1.414214   foo\r\n",
    "1 -0.507093 -0.707107   bar\r\n",
    "2  0.169031  0.000000   baz\r\n",
    "3  0.000000  0.707107   qux\r\n",
    "4  1.521278  1.414214  quux\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "602397c9-f8b9-4627-9601-cbf69610fade",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelencoder = LabelEncoder()\n",
    "df.iloc[:, -1] = labelencoder.fit_transform(df.iloc[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7aa8e28b-f06e-4581-aafe-c3d31eb20c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "0    22731\n",
       "3    19035\n",
       "5     7946\n",
       "2     2767\n",
       "6     2180\n",
       "1     1966\n",
       "4       36\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Label.value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "53347a93-868c-42a5-89c5-2dd4999d45fc",
   "metadata": {},
   "source": [
    "Original DataFrame:\n",
    "   Feature1 Feature2\n",
    "0         1        A\n",
    "1         2        B\n",
    "2         3        A\n",
    "3         4        C\n",
    "\n",
    "Transformed DataFrame:\n",
    "   Feature1  Feature2\n",
    "0         1         0\n",
    "1         2         1\n",
    "2         3         0\n",
    "3         4         2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ec9604-a523-4de1-9a03-c0c35f408f93",
   "metadata": {},
   "source": [
    "Here the minority class instances are of the indexes 6,1,4 (the last 3 instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfc821e-4a80-4ac4-989a-253b1e59817d",
   "metadata": {},
   "source": [
    "### split train set and test set using the sampled set"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ad17d384-5b6b-4cf6-ae5e-af038491bf77",
   "metadata": {},
   "source": [
    "# Read the sampled dataset\n",
    "df_sampled=pd.read_csv('./data/CICIDS2017_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "47af9610-68be-4cc0-9ba9-8c88959e848a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['Label'],axis=1).values\n",
    "y = df.iloc[:, -1].values.reshape(-1,1)\n",
    "y=np.ravel(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "74f6f97b-3878-4830-b2d8-83941a951d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size = 0.8, test_size = 0.2, random_state = 0,stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee10605-e51f-451a-8045-17e39697acf2",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da398fd0-d1b6-4490-b406-1461505cfbbe",
   "metadata": {},
   "source": [
    "### Feature selection by Information Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "47778e26-d3e6-432b-806c-10d8b77ff23a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: 'unknown'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mutual_info_classif\n\u001b[1;32m----> 2\u001b[0m importances \u001b[38;5;241m=\u001b[39m \u001b[43mmutual_info_classif\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_mutual_info.py:467\u001b[0m, in \u001b[0;36mmutual_info_classif\u001b[1;34m(X, y, discrete_features, n_neighbors, copy, random_state)\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmutual_info_classif\u001b[39m(\n\u001b[0;32m    392\u001b[0m     X, y, \u001b[38;5;241m*\u001b[39m, discrete_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m, n_neighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    393\u001b[0m ):\n\u001b[0;32m    394\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Estimate mutual information for a discrete target variable.\u001b[39;00m\n\u001b[0;32m    395\u001b[0m \n\u001b[0;32m    396\u001b[0m \u001b[38;5;124;03m    Mutual information (MI) [1]_ between two random variables is a non-negative\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;124;03m           of a Random Vector:, Probl. Peredachi Inf., 23:2 (1987), 9-16\u001b[39;00m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 467\u001b[0m     \u001b[43mcheck_classification_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _estimate_mi(X, y, discrete_features, \u001b[38;5;28;01mTrue\u001b[39;00m, n_neighbors, copy, random_state)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:218\u001b[0m, in \u001b[0;36mcheck_classification_targets\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m    210\u001b[0m y_type \u001b[38;5;241m=\u001b[39m type_of_target(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel-sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    217\u001b[0m ]:\n\u001b[1;32m--> 218\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown label type: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m y_type)\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown label type: 'unknown'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "importances = mutual_info_classif(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcff944-cc79-41a5-b561-e5706629ca6e",
   "metadata": {},
   "source": [
    "#### Interpretation\r\n",
    "Higher Scores: Features with higher mutual information scores are more informative about the target variable.\r\n",
    "Lower Scores: Features with lower scores provide less information about the targetity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b740b7-f7a0-41c6-9f2d-b033fa300cde",
   "metadata": {},
   "source": [
    "#### Use Case in Feature Selection\n",
    "Mutual information scores can be used to select a subset of features that are most relevant to the target variable. This can improve model performance by reducing overfitting and computational complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a2f1fe3-8783-4562-8288-1526454203a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the sum of importance scores\n",
    "f_list = sorted(zip(map(lambda x: round(x, 4), importances), features), reverse=True)\n",
    "Sum = 0\n",
    "fs = []\n",
    "for i in range(0, len(f_list)):\n",
    "    Sum = Sum + f_list[i][0]\n",
    "    fs.append(f_list[i][1])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "826daca0-e2e5-4042-bb26-2215352f6168",
   "metadata": {},
   "source": [
    "The code calculates the mutual information between each feature and the target variable.\n",
    "It sorts the features based on their importance scores in descending order.\n",
    "It then accumulates the total sum of the importance scores and collects the sorted feature names into a list.\n",
    "At the end of this process:\n",
    "\n",
    "Sum: The total sum of all the feature importance scores.\n",
    "fs: A list of feature names sorted by their importance scores in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e4b3f1f-e3b0-42d5-add1-7a5490297203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the important features from top to bottom until the accumulated importance reaches 90%\n",
    "f_list2 = sorted(zip(map(lambda x: round(x, 4), importances/Sum), features), reverse=True)\n",
    "Sum2 = 0\n",
    "fs = []\n",
    "for i in range(0, len(f_list2)):\n",
    "    Sum2 = Sum2 + f_list2[i][0]\n",
    "    fs.append(f_list2[i][1])\n",
    "    if Sum2>=0.9:\n",
    "        break        "
   ]
  },
  {
   "cell_type": "raw",
   "id": "addfdeca-080b-4e36-af04-d804275394ff",
   "metadata": {},
   "source": [
    "The code first normalizes the feature importance scores so that their sum equals 1.\n",
    "It then sorts the features based on their normalized importance scores in descending order.\n",
    "It accumulates the importance scores and selects features until the accumulated importance reaches 90%.\n",
    "At the end of this process:\n",
    "\n",
    "Sum2: The accumulated normalized importance score (it will be slightly above or exactly 0.9).\n",
    "fs: A list of feature names that together contribute to 90% of the total importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87295aab-3a46-4351-8fda-781f6e0096d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fs = df[fs].values\n",
    "##  X_fs_1 is to be checked without using the FCBF Filter\n",
    "X_fs_1=df[fs].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3204ac8f-448b-4b2b-bcb0-41230b109d89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56661, 45)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_fs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bf3fc0-cb6a-4f64-a2d3-a09af61c56f8",
   "metadata": {},
   "source": [
    "### Feature selection by Fast Correlation Based Filter (FCBF)\n",
    "\n",
    "This is a Moudule imported from GitHub repo: https://github.com/SantiagoEG/FCBF_module, which is a custom Module\n",
    "\n",
    "Certain features are redundant because they contain very similar information. FCBF can remove redundant features by calculating the correlation between each pair of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6d66406-5f76-4ba1-9d74-2cd1b615334d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FCBF_module import FCBF, FCBFK, FCBFiP, get_i\n",
    "fcbf = FCBFK(k = 20)\n",
    "#fcbf.fit(X_fs, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aea89eb3-0625-4cc6-b4fe-c7e3c0ea327e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fss = fcbf.fit_transform(X_fs,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "831a289a-bfba-40cb-a99e-e1c614b7089f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56661, 20)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_fss.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c0df73-aa31-4c66-bf1d-87a17107ba1c",
   "metadata": {},
   "source": [
    "## Re-split train & test sets after feature selection\n",
    "\n",
    "\n",
    "This is done including the FCBF Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e3a515ea-2f2c-4dd0-8479-2fde51591f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_fss,y, train_size = 0.8, test_size = 0.2, random_state = 0,stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75600d0f-f482-4e2b-89b0-30a3b12f0785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45328, 20)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f36aab9-b82e-4dac-a45f-b45701040b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BENIGN          18184\n",
       "DoS             15228\n",
       "PortScan         6357\n",
       "BruteForce       2213\n",
       "WebAttack        1744\n",
       "Bot              1573\n",
       "Infiltration       29\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_train).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d670957-6d8d-445a-b7ca-5d8f90457bb2",
   "metadata": {},
   "source": [
    "### SMOTE to solve class-imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "19a70be4-62e5-433e-a914-83c87ecaf4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote=SMOTE(n_jobs=-1,sampling_strategy={2:1000,4:1000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9d72718-2a13-4e96-8f7e-59e0c07dc005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes in y_train: ['BENIGN' 'Bot' 'BruteForce' 'DoS' 'Infiltration' 'PortScan' 'WebAttack']\n"
     ]
    }
   ],
   "source": [
    "unique_classes = np.unique(y_train)\n",
    "print(f\"Unique classes in y_train: {unique_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3d928a77-6c63-41ea-bed0-beeb96d132c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The {2, 4} target class is/are not present in the data.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m \u001b[43msmote\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\imblearn\\base.py:208\u001b[0m, in \u001b[0;36mBaseSampler.fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \n\u001b[0;32m    189\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;124;03m    The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m--> 208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\imblearn\\base.py:108\u001b[0m, in \u001b[0;36mSamplerMixin.fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    105\u001b[0m arrays_transformer \u001b[38;5;241m=\u001b[39m ArraysTransformer(X, y)\n\u001b[0;32m    106\u001b[0m X, y, binarize_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_X_y(X, y)\n\u001b[1;32m--> 108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy_ \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_sampling_strategy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampling_strategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sampling_type\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_resample(X, y)\n\u001b[0;32m    114\u001b[0m y_ \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    115\u001b[0m     label_binarize(output[\u001b[38;5;241m1\u001b[39m], classes\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39munique(y)) \u001b[38;5;28;01mif\u001b[39;00m binarize_y \u001b[38;5;28;01melse\u001b[39;00m output[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    116\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\imblearn\\utils\\_validation.py:536\u001b[0m, in \u001b[0;36mcheck_sampling_strategy\u001b[1;34m(sampling_strategy, y, sampling_type, **kwargs)\u001b[0m\n\u001b[0;32m    531\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m OrderedDict(\n\u001b[0;32m    532\u001b[0m         \u001b[38;5;28msorted\u001b[39m(SAMPLING_TARGET_KIND[sampling_strategy](y, sampling_type)\u001b[38;5;241m.\u001b[39mitems())\n\u001b[0;32m    533\u001b[0m     )\n\u001b[0;32m    534\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sampling_strategy, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    535\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m OrderedDict(\n\u001b[1;32m--> 536\u001b[0m         \u001b[38;5;28msorted\u001b[39m(\u001b[43m_sampling_strategy_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msampling_strategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_type\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitems())\n\u001b[0;32m    537\u001b[0m     )\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sampling_strategy, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m OrderedDict(\n\u001b[0;32m    540\u001b[0m         \u001b[38;5;28msorted\u001b[39m(_sampling_strategy_list(sampling_strategy, y, sampling_type)\u001b[38;5;241m.\u001b[39mitems())\n\u001b[0;32m    541\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\imblearn\\utils\\_validation.py:298\u001b[0m, in \u001b[0;36m_sampling_strategy_dict\u001b[1;34m(sampling_strategy, y, sampling_type)\u001b[0m\n\u001b[0;32m    294\u001b[0m set_diff_sampling_strategy_target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(sampling_strategy\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(\n\u001b[0;32m    295\u001b[0m     target_stats\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[0;32m    296\u001b[0m )\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(set_diff_sampling_strategy_target) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 298\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    299\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mset_diff_sampling_strategy_target\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m target class is/are not \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    300\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresent in the data.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    301\u001b[0m     )\n\u001b[0;32m    302\u001b[0m \u001b[38;5;66;03m# check that there is no negative number\u001b[39;00m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(n_samples \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m n_samples \u001b[38;5;129;01min\u001b[39;00m sampling_strategy\u001b[38;5;241m.\u001b[39mvalues()):\n",
      "\u001b[1;31mValueError\u001b[0m: The {2, 4} target class is/are not present in the data."
     ]
    }
   ],
   "source": [
    "X_train, y_train = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5462844f-6018-4c90-95d1-44b4e83a8f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y_train).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3062a7-c618-4a3b-80e8-9f0c7793b69e",
   "metadata": {},
   "source": [
    "## Machine learning model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d428861-3013-47d7-bdf8-43f4d52aeb4d",
   "metadata": {},
   "source": [
    "### XGBoost with FCBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140f95e7-b59a-4e74-9f94-01b5fda0c583",
   "metadata": {},
   "outputs": [],
   "source": [
    "xg = xgb.XGBClassifier(n_estimators = 10)\n",
    "xg.fit(X_train,y_train)\n",
    "xg_score=xg.score(X_test,y_test)\n",
    "y_predict=xg.predict(X_test)\n",
    "y_true=y_test\n",
    "print('Accuracy of XGBoost: '+ str(xg_score))\n",
    "precision,recall,fscore,none= precision_recall_fscore_support(y_true, y_predict, average='weighted') \n",
    "print('Precision of XGBoost: '+(str(precision)))\n",
    "print('Recall of XGBoost: '+(str(recall)))\n",
    "print('F1-score of XGBoost: '+(str(fscore)))\n",
    "print(classification_report(y_true,y_predict))\n",
    "cm=confusion_matrix(y_true,y_predict)\n",
    "f,ax=plt.subplots(figsize=(5,5))\n",
    "sns.heatmap(cm,annot=True,linewidth=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\n",
    "plt.xlabel(\"y_pred\")\n",
    "plt.ylabel(\"y_true\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8255bc4e-34b3-4e53-b47b-3f56ed855bef",
   "metadata": {},
   "source": [
    "#### Hyperparameter optimization (HPO) of XGBoost using Bayesian optimization with tree-based Parzen estimator (BO-TPE)\n",
    "Based on the GitHub repo for HPO: https://github.com/LiYangHart/Hyperparameter-Optimization-of-Machine-Learning-Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228e869b-4258-411c-81dd-339cefea0858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdadccc-8369-4fd2-8e15-4ca8f2f73581",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "def objective(params):\n",
    "    params = {\n",
    "        'n_estimators': int(params['n_estimators']), \n",
    "        'max_depth': int(params['max_depth']),\n",
    "        'learning_rate':  abs(float(params['learning_rate'])),\n",
    "\n",
    "    }\n",
    "    clf = xgb.XGBClassifier( **params)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    score = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    return {'loss':-score, 'status': STATUS_OK }\n",
    "\n",
    "space = {\n",
    "    'n_estimators': hp.quniform('n_estimators', 10, 100, 5),\n",
    "    'max_depth': hp.quniform('max_depth', 4, 100, 1),\n",
    "    'learning_rate': hp.normal('learning_rate', 0.01, 0.9),\n",
    "}\n",
    "\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=20)\n",
    "print(\"XGBoost: Hyperopt estimated optimum {}\".format(best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3d3037-9446-4990-976c-328f3607b4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "xg = xgb.XGBClassifier(learning_rate= 0.8391029526621202, n_estimators = 25, max_depth = 73)\n",
    "xg.fit(X_train,y_train)\n",
    "xg_score=xg.score(X_test,y_test)\n",
    "y_predict=xg.predict(X_test)\n",
    "y_true=y_test\n",
    "print('Accuracy of XGBoost: '+ str(xg_score))\n",
    "precision,recall,fscore,none= precision_recall_fscore_support(y_true, y_predict, average='weighted') \n",
    "print('Precision of XGBoost: '+(str(precision)))\n",
    "print('Recall of XGBoost: '+(str(recall)))\n",
    "print('F1-score of XGBoost: '+(str(fscore)))\n",
    "print(classification_report(y_true,y_predict))\n",
    "cm=confusion_matrix(y_true,y_predict)\n",
    "f,ax=plt.subplots(figsize=(5,5))\n",
    "sns.heatmap(cm,annot=True,linewidth=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\n",
    "plt.xlabel(\"y_pred\")\n",
    "plt.ylabel(\"y_true\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d169327c-e759-4fb7-a578-9db765c0f0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_train=xg.predict(X_train)\n",
    "xg_test=xg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570ecd3e-e754-453a-9f5f-3ec7b7970dc7",
   "metadata": {},
   "source": [
    "### XGBoost without FCBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bec5e27-e181-4fc7-b14d-7f7038318498",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X_fs_1,y, train_size = 0.8, test_size = 0.2, random_state = 0,stratify = y)\n",
    "pd.Series(y_train_1).value_counts()\n",
    "smote=SMOTE(n_jobs=-1,sampling_strategy={2:1000,4:1000})\n",
    "X_train_1, y_train_1 = smote.fit_resample(X_train_1, y_train_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcf478d-ebba-44f1-a5ab-209609a7b655",
   "metadata": {},
   "outputs": [],
   "source": [
    "xg = xgb.XGBClassifier(n_estimators = 10)\n",
    "xg.fit(X_train_1,y_train_1)\n",
    "xg_score=xg.score(X_test_1,y_test_1)\n",
    "y_predict_1=xg.predict(X_test_1)\n",
    "y_true_1=y_test_1\n",
    "print('Accuracy of XGBoost: '+ str(xg_score))\n",
    "precision,recall,fscore,none= precision_recall_fscore_support(y_true_1, y_predict_1, average='weighted') \n",
    "print('Precision of XGBoost: '+(str(precision)))\n",
    "print('Recall of XGBoost: '+(str(recall)))\n",
    "print('F1-score of XGBoost: '+(str(fscore)))\n",
    "print(classification_report(y_true,y_predict))\n",
    "cm=confusion_matrix(y_true_1,y_predict_1)\n",
    "f,ax=plt.subplots(figsize=(5,5))\n",
    "sns.heatmap(cm,annot=True,linewidth=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\n",
    "plt.xlabel(\"y_pred_1\")\n",
    "plt.ylabel(\"y_true_1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825ee1e2-4496-4af6-8f78-76746280a24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    params = {\n",
    "        'n_estimators': int(params['n_estimators']), \n",
    "        'max_depth': int(params['max_depth']),\n",
    "        'learning_rate':  abs(float(params['learning_rate'])),\n",
    "\n",
    "    }\n",
    "    clf = xgb.XGBClassifier( **params)\n",
    "    clf.fit(X_train_1, y_train_1)\n",
    "    y_pred_1 = clf.predict(X_test_1)\n",
    "    score = accuracy_score(y_test_1, y_pred_1)\n",
    "\n",
    "    return {'loss':-score, 'status': STATUS_OK }\n",
    "\n",
    "\n",
    "space = {\n",
    "    'n_estimators': hp.quniform('n_estimators', 10, 100, 5),\n",
    "    'max_depth': hp.quniform('max_depth', 4, 100, 1),\n",
    "    'learning_rate': hp.normal('learning_rate', 0.01, 0.9),\n",
    "}\n",
    "\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=20)\n",
    "print(\"XGBoost: Hyperopt estimated optimum {}\".format(best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7bc5d1-7ec9-4e9f-8942-fa4754b81f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "xg = xgb.XGBClassifier(learning_rate= 0.1206862326628798, n_estimators = 55, max_depth = 75)\n",
    "xg.fit(X_train_1,y_train_1)\n",
    "xg_score=xg.score(X_test_1,y_test_1)\n",
    "y_predict=xg.predict(X_test_1)\n",
    "y_true_1=y_test_1\n",
    "print('Accuracy of XGBoost: '+ str(xg_score))\n",
    "precision,recall,fscore,none= precision_recall_fscore_support(y_true_1, y_predict_1, average='weighted') \n",
    "print('Precision of XGBoost: '+(str(precision)))\n",
    "print('Recall of XGBoost: '+(str(recall)))\n",
    "print('F1-score of XGBoost: '+(str(fscore)))\n",
    "print(classification_report(y_true_1,y_predict_1))\n",
    "cm=confusion_matrix(y_true_1,y_predict_1)\n",
    "f,ax=plt.subplots(figsize=(5,5))\n",
    "sns.heatmap(cm,annot=True,linewidth=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\n",
    "plt.xlabel(\"y_pred\")\n",
    "plt.ylabel(\"y_true\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe76de5-6ed3-478f-a47a-e08297fb13d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_train_1=xg.predict(X_train_1)\n",
    "xg_test_1=xg.predict(X_test_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d3155d-ea74-4333-86fc-339241fdaa6b",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bbecbd-7774-4bd0-9c7d-9ae103476a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_fss,y, train_size = 0.8, test_size = 0.2, random_state = 0,stratify = y)\n",
    "smote=SMOTE(n_jobs=-1,sampling_strategy={2:1000,4:1000})\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe85535b-ce69-4efb-a393-bc43185855af",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state = 0)\n",
    "rf.fit(X_train,y_train) \n",
    "rf_score=rf.score(X_test,y_test)\n",
    "y_predict=rf.predict(X_test)\n",
    "y_true=y_test\n",
    "print('Accuracy of RF: '+ str(rf_score))\n",
    "precision,recall,fscore,none= precision_recall_fscore_support(y_true, y_predict, average='weighted') \n",
    "print('Precision of RF: '+(str(precision)))\n",
    "print('Recall of RF: '+(str(recall)))\n",
    "print('F1-score of RF: '+(str(fscore)))\n",
    "print(classification_report(y_true,y_predict))\n",
    "cm=confusion_matrix(y_true,y_predict)\n",
    "f,ax=plt.subplots(figsize=(5,5))\n",
    "sns.heatmap(cm,annot=True,linewidth=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\n",
    "plt.xlabel(\"y_pred\")\n",
    "plt.ylabel(\"y_true\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b61a367-4dc9-4786-b38c-90e9d5b8b80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter optimization of random forest\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "# Define the objective function\n",
    "def objective(params):\n",
    "    params = {\n",
    "        'n_estimators': int(params['n_estimators']), \n",
    "        'max_depth': int(params['max_depth']),\n",
    "        'max_features': int(params['max_features']),\n",
    "        \"min_samples_split\":int(params['min_samples_split']),\n",
    "        \"min_samples_leaf\":int(params['min_samples_leaf']),\n",
    "        \"criterion\":str(params['criterion'])\n",
    "    }\n",
    "    clf = RandomForestClassifier( **params)\n",
    "    clf.fit(X_train,y_train)\n",
    "    score=clf.score(X_test,y_test)\n",
    "\n",
    "    return {'loss':-score, 'status': STATUS_OK }\n",
    "# Define the hyperparameter configuration space\n",
    "space = {\n",
    "    'n_estimators': hp.quniform('n_estimators', 10, 200, 1),\n",
    "    'max_depth': hp.quniform('max_depth', 5, 50, 1),\n",
    "    \"max_features\":hp.quniform('max_features', 1, 20, 1),\n",
    "    \"min_samples_split\":hp.quniform('min_samples_split',2,11,1),\n",
    "    \"min_samples_leaf\":hp.quniform('min_samples_leaf',1,11,1),\n",
    "    \"criterion\":hp.choice('criterion',['gini','entropy'])\n",
    "}\n",
    "\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=20)\n",
    "print(\"Random Forest: Hyperopt estimated optimum {}\".format(best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c240aa64-e8e9-4a65-8546-03f6373cf106",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_hpo = RandomForestClassifier(n_estimators = 27, min_samples_leaf = 1, max_depth = 45, min_samples_split = 2, max_features = 8, criterion = 'entropy')\n",
    "rf_hpo.fit(X_train,y_train)\n",
    "rf_score=rf_hpo.score(X_test,y_test)\n",
    "y_predict=rf_hpo.predict(X_test)\n",
    "y_true=y_test\n",
    "print('Accuracy of RF: '+ str(rf_score))\n",
    "precision,recall,fscore,none= precision_recall_fscore_support(y_true, y_predict, average='weighted') \n",
    "print('Precision of RF: '+(str(precision)))\n",
    "print('Recall of RF: '+(str(recall)))\n",
    "print('F1-score of RF: '+(str(fscore)))\n",
    "print(classification_report(y_true,y_predict))\n",
    "cm=confusion_matrix(y_true,y_predict)\n",
    "f,ax=plt.subplots(figsize=(5,5))\n",
    "sns.heatmap(cm,annot=True,linewidth=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\n",
    "plt.xlabel(\"y_pred\")\n",
    "plt.ylabel(\"y_true\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d2274d-014a-4ba5-b0c9-aa8610908fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_train=rf_hpo.predict(X_train)\n",
    "rf_test=rf_hpo.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e500d276-4ea2-47cc-9835-0d5e5480e252",
   "metadata": {},
   "source": [
    "#### Random Forest without FCBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36bfc29-5a6b-4e8a-b350-7ec274d0efce",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X_fs_1,y, train_size = 0.8, test_size = 0.2, random_state = 0,stratify = y)\n",
    "pd.Series(y_train_1).value_counts()\n",
    "smote=SMOTE(n_jobs=-1,sampling_strategy={2:1000,4:1000})\n",
    "X_train_1, y_train_1 = smote.fit_resample(X_train_1, y_train_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323dc16d-62ae-4f57-9ed3-573bf12e4f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state = 0)\n",
    "rf.fit(X_train_1,y_train_1) \n",
    "rf_score=rf.score(X_test_1,y_test_1)\n",
    "y_predict=rf.predict(X_test_1)\n",
    "y_true_1=y_test_1\n",
    "print('Accuracy of RF: '+ str(rf_score))\n",
    "precision,recall,fscore,none= precision_recall_fscore_support(y_true_1, y_predict_1, average='weighted') \n",
    "print('Precision of RF: '+(str(precision)))\n",
    "print('Recall of RF: '+(str(recall)))\n",
    "print('F1-score of RF: '+(str(fscore)))\n",
    "print(classification_report(y_true_1,y_predict_1))\n",
    "cm=confusion_matrix(y_true_1,y_predict_1)\n",
    "f,ax=plt.subplots(figsize=(5,5))\n",
    "sns.heatmap(cm,annot=True,linewidth=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\n",
    "plt.xlabel(\"y_pred\")\n",
    "plt.ylabel(\"y_true\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088190bc-b25c-4254-9bef-214bad0b636c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter optimization of random forest\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "# Define the objective function\n",
    "def objective(params):\n",
    "    params = {\n",
    "        'n_estimators': int(params['n_estimators']), \n",
    "        'max_depth': int(params['max_depth']),\n",
    "        'max_features': int(params['max_features']),\n",
    "        \"min_samples_split\":int(params['min_samples_split']),\n",
    "        \"min_samples_leaf\":int(params['min_samples_leaf']),\n",
    "        \"criterion\":str(params['criterion'])\n",
    "    }\n",
    "    clf = RandomForestClassifier( **params)\n",
    "    clf.fit(X_train_1,y_train_1)\n",
    "    score=clf.score(X_test_1,y_test_1)\n",
    "\n",
    "    return {'loss':-score, 'status': STATUS_OK }\n",
    "# Define the hyperparameter configuration space\n",
    "space = {\n",
    "    'n_estimators': hp.quniform('n_estimators', 10, 200, 1),\n",
    "    'max_depth': hp.quniform('max_depth', 5, 50, 1),\n",
    "    \"max_features\":hp.quniform('max_features', 1, 20, 1),\n",
    "    \"min_samples_split\":hp.quniform('min_samples_split',2,11,1),\n",
    "    \"min_samples_leaf\":hp.quniform('min_samples_leaf',1,11,1),\n",
    "    \"criterion\":hp.choice('criterion',['gini','entropy'])\n",
    "}\n",
    "\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=20)\n",
    "print(\"Random Forest: Hyperopt estimated optimum {}\".format(best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857a6109-221c-47d1-9976-07287ad8eccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_hpo = RandomForestClassifier(n_estimators = 146, min_samples_leaf = 1, max_depth = 39, min_samples_split = 8, max_features = 15, criterion = 'entropy')\n",
    "rf_hpo.fit(X_train_1,y_train_1)\n",
    "rf_score=rf_hpo.score(X_test_1,y_test_1)\n",
    "y_predict_1=rf_hpo.predict(X_test_1)\n",
    "y_true_1=y_test_1\n",
    "print('Accuracy of RF: '+ str(rf_score))\n",
    "precision,recall,fscore,none= precision_recall_fscore_support(y_true_1, y_predict_1, average='weighted') \n",
    "print('Precision of RF: '+(str(precision)))\n",
    "print('Recall of RF: '+(str(recall)))\n",
    "print('F1-score of RF: '+(str(fscore)))\n",
    "print(classification_report(y_true_1,y_predict_1))\n",
    "cm=confusion_matrix(y_true_1,y_predict_1)\n",
    "f,ax=plt.subplots(figsize=(5,5))\n",
    "sns.heatmap(cm,annot=True,linewidth=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\n",
    "plt.xlabel(\"y_pred\")\n",
    "plt.ylabel(\"y_true\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564369a9-e84b-42d2-b25d-0079c92b5dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_train_1=rf_hpo.predict(X_train_1)\n",
    "rf_test_1=rf_hpo.predict(X_test_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd6e6e7-7c99-4504-8851-a9883a25e1bc",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b4fde3-558c-4904-9981-0a87c9a54ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(random_state = 0)\n",
    "dt.fit(X_train,y_train) \n",
    "dt_score=dt.score(X_test,y_test)\n",
    "y_predict=dt.predict(X_test)\n",
    "y_true=y_test\n",
    "print('Accuracy of DT: '+ str(dt_score))\n",
    "precision,recall,fscore,none= precision_recall_fscore_support(y_true, y_predict, average='weighted') \n",
    "print('Precision of DT: '+(str(precision)))\n",
    "print('Recall of DT: '+(str(recall)))\n",
    "print('F1-score of DT: '+(str(fscore)))\n",
    "print(classification_report(y_true,y_predict))\n",
    "cm=confusion_matrix(y_true,y_predict)\n",
    "f,ax=plt.subplots(figsize=(5,5))\n",
    "sns.heatmap(cm,annot=True,linewidth=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\n",
    "plt.xlabel(\"y_pred\")\n",
    "plt.ylabel(\"y_true\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2659d6-3e09-4a08-943d-c1b6840e58ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter optimization of decision tree\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "# Define the objective function\n",
    "def objective(params):\n",
    "    params = {\n",
    "        'max_depth': int(params['max_depth']),\n",
    "        'max_features': int(params['max_features']),\n",
    "        \"min_samples_split\":int(params['min_samples_split']),\n",
    "        \"min_samples_leaf\":int(params['min_samples_leaf']),\n",
    "        \"criterion\":str(params['criterion'])\n",
    "    }\n",
    "    clf = DecisionTreeClassifier( **params)\n",
    "    clf.fit(X_train,y_train)\n",
    "    score=clf.score(X_test,y_test)\n",
    "\n",
    "    return {'loss':-score, 'status': STATUS_OK }\n",
    "# Define the hyperparameter configuration space\n",
    "space = {\n",
    "    'max_depth': hp.quniform('max_depth', 5, 50, 1),\n",
    "    \"max_features\":hp.quniform('max_features', 1, 20, 1),\n",
    "    \"min_samples_split\":hp.quniform('min_samples_split',2,11,1),\n",
    "    \"min_samples_leaf\":hp.quniform('min_samples_leaf',1,11,1),\n",
    "    \"criterion\":hp.choice('criterion',['gini','entropy'])\n",
    "}\n",
    "\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=50)\n",
    "print(\"Decision tree: Hyperopt estimated optimum {}\".format(best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e58394-d6ce-44fc-bb13-9a38662dd1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_hpo = DecisionTreeClassifier(min_samples_leaf = 4, max_depth = 17, min_samples_split = 8, max_features = 20, criterion = 'entropy')\n",
    "dt_hpo.fit(X_train,y_train)\n",
    "dt_score=dt_hpo.score(X_test,y_test)\n",
    "y_predict=dt_hpo.predict(X_test)\n",
    "y_true=y_test\n",
    "print('Accuracy of DT: '+ str(dt_score))\n",
    "precision,recall,fscore,none= precision_recall_fscore_support(y_true, y_predict, average='weighted') \n",
    "print('Precision of DT: '+(str(precision)))\n",
    "print('Recall of DT: '+(str(recall)))\n",
    "print('F1-score of DT: '+(str(fscore)))\n",
    "print(classification_report(y_true,y_predict))\n",
    "cm=confusion_matrix(y_true,y_predict)\n",
    "f,ax=plt.subplots(figsize=(5,5))\n",
    "sns.heatmap(cm,annot=True,linewidth=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\n",
    "plt.xlabel(\"y_pred\")\n",
    "plt.ylabel(\"y_true\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6e715d-a5e5-445b-b717-242131cbbdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_train=dt_hpo.predict(X_train)\n",
    "dt_test=dt_hpo.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10451df1-d645-4021-82e9-3b5151f45e26",
   "metadata": {},
   "source": [
    "#### DF without FCBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384ddaea-f8fa-4f64-92e8-298fcd9f6824",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(random_state = 0)\n",
    "dt.fit(X_train_1,y_train_1) \n",
    "dt_score=dt.score(X_test_1,y_test_1)\n",
    "y_predict_1=dt.predict(X_test_1)\n",
    "y_true_1=y_test_1\n",
    "print('Accuracy of DT: '+ str(dt_score))\n",
    "precision,recall,fscore,none= precision_recall_fscore_support(y_true_1, y_predict_1, average='weighted') \n",
    "print('Precision of DT: '+(str(precision)))\n",
    "print('Recall of DT: '+(str(recall)))\n",
    "print('F1-score of DT: '+(str(fscore)))\n",
    "print(classification_report(y_true_1,y_predict_1))\n",
    "cm=confusion_matrix(y_true_1,y_predict_1)\n",
    "f,ax=plt.subplots(figsize=(5,5))\n",
    "sns.heatmap(cm,annot=True,linewidth=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\n",
    "plt.xlabel(\"y_pred\")\n",
    "plt.ylabel(\"y_true\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e426db3-4a05-43b3-a47c-7684d2ea2550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter optimization of decision tree\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "# Define the objective function\n",
    "def objective(params):\n",
    "    params = {\n",
    "        'max_depth': int(params['max_depth']),\n",
    "        'max_features': int(params['max_features']),\n",
    "        \"min_samples_split\":int(params['min_samples_split']),\n",
    "        \"min_samples_leaf\":int(params['min_samples_leaf']),\n",
    "        \"criterion\":str(params['criterion'])\n",
    "    }\n",
    "    clf = DecisionTreeClassifier( **params)\n",
    "    clf.fit(X_train_1,y_train_1)\n",
    "    score=clf.score(X_test_1,y_test_1)\n",
    "\n",
    "    return {'loss':-score, 'status': STATUS_OK }\n",
    "# Define the hyperparameter configuration space\n",
    "space = {\n",
    "    'max_depth': hp.quniform('max_depth', 5, 50, 1),\n",
    "    \"max_features\":hp.quniform('max_features', 1, 20, 1),\n",
    "    \"min_samples_split\":hp.quniform('min_samples_split',2,11,1),\n",
    "    \"min_samples_leaf\":hp.quniform('min_samples_leaf',1,11,1),\n",
    "    \"criterion\":hp.choice('criterion',['gini','entropy'])\n",
    "}\n",
    "\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=50)\n",
    "print(\"Decision tree: Hyperopt estimated optimum {}\".format(best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f477619-15fb-4fea-9563-e724dc2c1b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_hpo = DecisionTreeClassifier(min_samples_leaf = 2, max_depth = 34, min_samples_split = 9, max_features = 12, criterion = 'entropy')\n",
    "dt_hpo.fit(X_train_1,y_train_1)\n",
    "dt_score=dt_hpo.score(X_test_1,y_test_1)\n",
    "y_predict_1=dt_hpo.predict(X_test_1)\n",
    "y_true_1=y_test_1\n",
    "print('Accuracy of DT: '+ str(dt_score))\n",
    "precision,recall,fscore,none= precision_recall_fscore_support(y_true_1, y_predict_1, average='weighted') \n",
    "print('Precision of DT: '+(str(precision)))\n",
    "print('Recall of DT: '+(str(recall)))\n",
    "print('F1-score of DT: '+(str(fscore)))\n",
    "print(classification_report(y_true_1,y_predict_1))\n",
    "cm=confusion_matrix(y_true_1,y_predict_1)\n",
    "f,ax=plt.subplots(figsize=(5,5))\n",
    "sns.heatmap(cm,annot=True,linewidth=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\n",
    "plt.xlabel(\"y_pred\")\n",
    "plt.ylabel(\"y_true\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b0c74e-dfbe-46a1-a1ea-bcd71e53a6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_train_1=dt_hpo.predict(X_train_1)\n",
    "dt_test_1=dt_hpo.predict(X_test_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171c0e61-ad1c-45bd-a5a4-6d924e778e37",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058184d0-1c72-46e8-90c3-a534a230b1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense"
   ]
  },
  {
   "cell_type": "raw",
   "id": "99086d9e-c4c7-48a7-8511-7487089c2407",
   "metadata": {},
   "source": [
    "keras=tf.keras\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "                                 keras.layers.Conv1D(32, kernel_size= 4, strides = 2, \n",
    "                                                     padding = 'valid'),\n",
    "                                 keras.layers.Conv1D(64, kernel_size=4, strides = 2, padding= 'valid'), \n",
    "                                 keras.layers.MaxPooling1D(), \n",
    "                                 keras.layers.Conv1D(128, kernel_size= 4, strides= 2), \n",
    "                                 keras.layers.Flatten(), \n",
    "                                 keras.layers.Dense(80, activation= 'relu'), \n",
    "                                 keras.layers.Dense(45, activation='relu'), \n",
    "                                 keras.layers.Dense(13, activation= 'softmax')\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fe58dad9-3d8f-40dd-ad9b-f556bc57f982",
   "metadata": {},
   "source": [
    "num_features = X_train.shape[1]\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "\n",
    "# Create a simple fully connected neural network (FCNN)\n",
    "fcnn = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(num_features,)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "cnn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "cnn.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "cnn_score = cnn.evaluate(X_test, y_test, verbose=0)\n",
    "print('Accuracy of CNN: ' + str(cnn_score[1]))\n",
    "\n",
    "# Predict using the model\n",
    "y_predict = np.argmax(cnn.predict(X_test), axis=-1)\n",
    "\n",
    "# Calculate precision, recall, and f1-score\n",
    "precision, recall, fscore, _ = precision_recall_fscore_support(y_test, y_predict, average='weighted')\n",
    "print('Precision of CNN: ' + str(precision))\n",
    "print('Recall of CNN: ' + str(recall))\n",
    "print('F1-score of CNN: ' + str(fscore))\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_predict))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_predict)\n",
    "f, ax = plt.subplots(figsize=(5, 5))\n",
    "sns.heatmap(cm, annot=True, linewidths=0.5, linecolor=\"red\", fmt=\".0f\", ax=ax)\n",
    "plt.xlabel(\"y_pred\")\n",
    "plt.ylabel(\"y_true\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b075a129-0f09-4378-8be7-af972b0ac987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data for CNN input (assuming your data is not in image format)\n",
    "X_train_cnn = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test_cnn = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# Create and compile CNN model\n",
    "model_cnn = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv1D(64, 3, activation='relu', input_shape=(X_train_cnn.shape[1], 1)),\n",
    "    tf.keras.layers.MaxPooling1D(2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train CNN model\n",
    "model_cnn.fit(X_train_cnn, y_train, epochs=10, validation_data=(X_test_cnn, y_test))\n",
    "\n",
    "# Evaluate CNN model\n",
    "cnn_score = model_cnn.evaluate(X_test_cnn, y_test, verbose=0)\n",
    "print('Accuracy of CNN: ' + str(cnn_score[1]))\n",
    "\n",
    "# Predict using the model\n",
    "y_predict_cnn = (model_cnn.predict(X_test_cnn) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision_cnn, recall_cnn, fscore_cnn, _ = precision_recall_fscore_support(y_test, y_predict_cnn, average='weighted')\n",
    "print('Precision of CNN: ' + str(precision_cnn))\n",
    "print('Recall of CNN: ' + str(recall_cnn))\n",
    "print('F1-score of CNN: ' + str(fscore_cnn))\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_predict_cnn))\n",
    "\n",
    "# Confusion matrix\n",
    "cm_cnn = confusion_matrix(y_test, y_predict_cnn)\n",
    "f, ax = plt.subplots(figsize=(5, 5))\n",
    "sns.heatmap(cm_cnn, annot=True, linewidths=0.5, linecolor=\"red\", fmt=\".0f\", ax=ax)\n",
    "plt.xlabel(\"y_pred\")\n",
    "plt.ylabel(\"y_true\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9090763b-3c53-4932-97e6-86b8c3dd90b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function for CNN\n",
    "def cnn_objective(params):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv1D(filters=int(params['filters']), kernel_size=int(params['kernel_size']), activation='relu', input_shape=(X_train.shape[1], 1)),\n",
    "        tf.keras.layers.MaxPooling1D(pool_size=int(params['pool_size'])),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(int(params['dense_units']), activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X_train_cnn, y_train, epochs=int(params['epochs']), validation_data=(X_test_cnn, y_test), verbose=0)\n",
    "    _, accuracy = model.evaluate(X_test_cnn, y_test, verbose=0)\n",
    "    return {'loss': -accuracy, 'status': STATUS_OK}\n",
    "\n",
    "# Define the hyperparameter configuration space for CNN\n",
    "cnn_space = {\n",
    "    'filters': hp.quniform('filters', 32, 128, 1),\n",
    "    'kernel_size': hp.quniform('kernel_size', 3, 10, 1),\n",
    "    'pool_size': hp.quniform('pool_size', 2, 5, 1),\n",
    "    'dense_units': hp.quniform('dense_units', 32, 128, 1),\n",
    "    'epochs': hp.quniform('epochs', 5, 20, 1)\n",
    "}\n",
    "\n",
    "# Perform hyperparameter optimization for CNN\n",
    "cnn_trials = Trials()\n",
    "best_cnn = fmin(fn=cnn_objective, space=cnn_space, algo=tpe.suggest, max_evals=50, trials=cnn_trials)\n",
    "\n",
    "print(\"CNN: Hyperopt estimated optimum {}\".format(best_cnn))\n",
    "\n",
    "# Extract the best hyperparameters for CNN\n",
    "best_filters = int(best_cnn['filters'])\n",
    "best_kernel_size = int(best_cnn['kernel_size'])\n",
    "best_pool_size = int(best_cnn['pool_size'])\n",
    "best_dense_units = int(best_cnn['dense_units'])\n",
    "best_epochs = int(best_cnn['epochs'])\n",
    "\n",
    "# Create and train CNN model with best hyperparameters\n",
    "cnn_hpo = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv1D(filters=best_filters, kernel_size=best_kernel_size, activation='relu', input_shape=(X_train_cnn.shape[1], 1)),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=best_pool_size),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(best_dense_units, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "cnn_hpo.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "cnn_hpo.fit(X_train_cnn, y_train, epochs=best_epochs, validation_data=(X_test_cnn, y_test))\n",
    "\n",
    "# Evaluate CNN model with best hyperparameters\n",
    "cnn_score = cnn_hpo.evaluate(X_test_cnn, y_test, verbose=0)\n",
    "print('Accuracy of CNN with best hyperparameters: ' + str(cnn_score[1]))\n",
    "\n",
    "# Predict using the CNN model with best hyperparameters\n",
    "y_predict_cnn = (cnn_hpo.predict(X_test_cnn) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Calculate precision, recall, and F1-score for CNN model\n",
    "precision_cnn, recall_cnn, fscore_cnn, _ = precision_recall_fscore_support(y_test, y_predict_cnn, average='weighted')\n",
    "print('Precision of CNN with best hyperparameters: ' + str(precision_cnn))\n",
    "print('Recall of CNN with best hyperparameters: ' + str(recall_cnn))\n",
    "print('F1-score of CNN with best hyperparameters: ' + str(fscore_cnn))\n",
    "\n",
    "# Print classification report for CNN model\n",
    "print(classification_report(y_test, y_predict_cnn))\n",
    "\n",
    "# Confusion matrix for CNN model\n",
    "cm_cnn = confusion_matrix(y_test, y_predict_cnn)\n",
    "f, ax = plt.subplots(figsize=(5, 5))\n",
    "sns.heatmap(cm_cnn, annot=True, linewidths=0.5, linecolor=\"red\", fmt=\".0f\", ax=ax)\n",
    "plt.xlabel(\"y_pred\")\n",
    "plt.ylabel(\"y_true\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60523290-88ec-426f-9288-1e1239fff07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_train=cnn_hpo.predict(X_train_cnn)\n",
    "cnn_test=cnn_hpo.predict(X_test_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b02436-8cec-419b-a234-c721b01fc1ec",
   "metadata": {},
   "source": [
    "#### CNN without FCBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302c74c8-2176-4e07-8208-7f424e431e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data for CNN input (assuming your data is not in image format)\n",
    "X_train_cnn_1 = X_train_1.reshape(X_train_1.shape[0], X_train_1.shape[1], 1)\n",
    "X_test_cnn_1 = X_test_1.reshape(X_test_1.shape[0], X_test_1.shape[1], 1)\n",
    "\n",
    "# Create and compile CNN model\n",
    "model_cnn = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv1D(64, 3, activation='relu', input_shape=(X_train_cnn_1.shape[1], 1)),\n",
    "    tf.keras.layers.MaxPooling1D(2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train CNN model\n",
    "model_cnn.fit(X_train_cnn_1, y_train_1, epochs=10, validation_data=(X_test_cnn_1, y_test_1))\n",
    "\n",
    "# Evaluate CNN model\n",
    "cnn_score = model_cnn.evaluate(X_test_cnn_1, y_test_1, verbose=0)\n",
    "print('Accuracy of CNN: ' + str(cnn_score[1]))\n",
    "\n",
    "# Predict using the model\n",
    "y_predict_cnn_1 = (model_cnn.predict(X_test_cnn_1) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision_cnn, recall_cnn, fscore_cnn, _ = precision_recall_fscore_support(y_test_1, y_predict_cnn_1, average='weighted')\n",
    "print('Precision of CNN: ' + str(precision_cnn))\n",
    "print('Recall of CNN: ' + str(recall_cnn))\n",
    "print('F1-score of CNN: ' + str(fscore_cnn))\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test_1, y_predict_cnn_1))\n",
    "\n",
    "# Confusion matrix\n",
    "cm_cnn = confusion_matrix(y_test_1, y_predict_cnn_1)\n",
    "f, ax = plt.subplots(figsize=(5, 5))\n",
    "sns.heatmap(cm_cnn, annot=True, linewidths=0.5, linecolor=\"red\", fmt=\".0f\", ax=ax)\n",
    "plt.xlabel(\"y_pred\")\n",
    "plt.ylabel(\"y_true\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f71ac0d-706a-48c3-a7b1-aa8b6364d85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function for CNN\n",
    "def cnn_objective(params):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv1D(filters=int(params['filters']), kernel_size=int(params['kernel_size']), activation='relu', input_shape=(X_train_1.shape[1], 1)),\n",
    "        tf.keras.layers.MaxPooling1D(pool_size=int(params['pool_size'])),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(int(params['dense_units']), activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X_train_cnn_1, y_train_1, epochs=int(params['epochs']), validation_data=(X_test_cnn_1, y_test_1), verbose=0)\n",
    "    _, accuracy = model.evaluate(X_test_cnn_1, y_test_1, verbose=0)\n",
    "    return {'loss': -accuracy, 'status': STATUS_OK}\n",
    "\n",
    "# Define the hyperparameter configuration space for CNN\n",
    "cnn_space = {\n",
    "    'filters': hp.quniform('filters', 32, 128, 1),\n",
    "    'kernel_size': hp.quniform('kernel_size', 3, 10, 1),\n",
    "    'pool_size': hp.quniform('pool_size', 2, 5, 1),\n",
    "    'dense_units': hp.quniform('dense_units', 32, 128, 1),\n",
    "    'epochs': hp.quniform('epochs', 5, 20, 1)\n",
    "}\n",
    "\n",
    "# Perform hyperparameter optimization for CNN\n",
    "cnn_trials = Trials()\n",
    "best_cnn = fmin(fn=cnn_objective, space=cnn_space, algo=tpe.suggest, max_evals=50, trials=cnn_trials)\n",
    "\n",
    "print(\"CNN: Hyperopt estimated optimum {}\".format(best_cnn))\n",
    "\n",
    "# Extract the best hyperparameters for CNN\n",
    "best_filters = int(best_cnn['filters'])\n",
    "best_kernel_size = int(best_cnn['kernel_size'])\n",
    "best_pool_size = int(best_cnn['pool_size'])\n",
    "best_dense_units = int(best_cnn['dense_units'])\n",
    "best_epochs = int(best_cnn['epochs'])\n",
    "\n",
    "# Create and train CNN model with best hyperparameters\n",
    "cnn_hpo = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv1D(filters=best_filters, kernel_size=best_kernel_size, activation='relu', input_shape=(X_train_cnn_1.shape[1], 1)),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=best_pool_size),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(best_dense_units, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "cnn_hpo.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "cnn_hpo.fit(X_train_cnn_1, y_train_1, epochs=best_epochs, validation_data=(X_test_cnn_1, y_test_1))\n",
    "\n",
    "# Evaluate CNN model with best hyperparameters\n",
    "cnn_score = cnn_hpo.evaluate(X_test_cnn_1, y_test_1, verbose=0)\n",
    "print('Accuracy of CNN with best hyperparameters: ' + str(cnn_score[1]))\n",
    "\n",
    "# Predict using the CNN model with best hyperparameters\n",
    "y_predict_cnn_1 = (cnn_hpo.predict(X_test_cnn_1) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Calculate precision, recall, and F1-score for CNN model\n",
    "precision_cnn, recall_cnn, fscore_cnn, _ = precision_recall_fscore_support(y_test_1, y_predict_cnn_1, average='weighted')\n",
    "print('Precision of CNN with best hyperparameters: ' + str(precision_cnn))\n",
    "print('Recall of CNN with best hyperparameters: ' + str(recall_cnn))\n",
    "print('F1-score of CNN with best hyperparameters: ' + str(fscore_cnn))\n",
    "\n",
    "# Print classification report for CNN model\n",
    "print(classification_report(y_test_1, y_predict_cnn_1))\n",
    "\n",
    "# Confusion matrix for CNN model\n",
    "cm_cnn = confusion_matrix(y_test_1, y_predict_cnn_1)\n",
    "f, ax = plt.subplots(figsize=(5, 5))\n",
    "sns.heatmap(cm_cnn, annot=True, linewidths=0.5, linecolor=\"red\", fmt=\".0f\", ax=ax)\n",
    "plt.xlabel(\"y_pred\")\n",
    "plt.ylabel(\"y_true\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042dca2d-789b-4701-9ebc-8975d1352ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_train_1=cnn_hpo.predict(X_train_cnn_1)\n",
    "cnn_test_1=cnn_hpo.predict(X_test_cnn_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90301f4-7791-437b-b318-9dbf97a77960",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2676eb37-d4a6-479e-a8f7-36725c63db4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_lstm = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(64, input_shape=(X_train.shape[1], 1)),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train LSTM model\n",
    "model_lstm.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate LSTM model\n",
    "lstm_score = model_lstm.evaluate(X_test, y_test, verbose=0)\n",
    "print('Accuracy of LSTM: ' + str(lstm_score[1]))\n",
    "\n",
    "# Predict using the model\n",
    "y_predict_lstm = (model_lstm.predict(X_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision_lstm, recall_lstm, fscore_lstm, _ = precision_recall_fscore_support(y_test, y_predict_lstm, average='weighted')\n",
    "print('Precision of LSTM: ' + str(precision_lstm))\n",
    "print('Recall of LSTM: ' + str(recall_lstm))\n",
    "print('F1-score of LSTM: ' + str(fscore_lstm))\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_predict_lstm))\n",
    "\n",
    "# Confusion matrix\n",
    "cm_lstm = confusion_matrix(y_test, y_predict_lstm)\n",
    "f, ax = plt.subplots(figsize=(5, 5))\n",
    "sns.heatmap(cm_lstm, annot=True, linewidths=0.5, linecolor=\"red\", fmt=\".0f\", ax=ax)\n",
    "plt.xlabel(\"y_pred\")\n",
    "plt.ylabel(\"y_true\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e94377-3380-4325-8879-97a96187de5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function for LSTM\n",
    "def lstm_objective(params):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.LSTM(units=int(params['units']), input_shape=(X_train.shape[1], 1)),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, epochs=int(params['epochs']), validation_data=(X_test, y_test), verbose=0)\n",
    "    _, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    return {'loss': -accuracy, 'status': STATUS_OK}\n",
    "\n",
    "# Define the hyperparameter configuration space for LSTM\n",
    "lstm_space = {\n",
    "    'units': hp.quniform('units', 32, 128, 1),\n",
    "    'epochs': hp.quniform('epochs', 5, 20, 1)\n",
    "}\n",
    "\n",
    "# Perform hyperparameter optimization for LSTM\n",
    "lstm_trials = Trials()\n",
    "best_lstm = fmin(fn=lstm_objective, space=lstm_space, algo=tpe.suggest, max_evals=50, trials=lstm_trials)\n",
    "\n",
    "print(\"LSTM: Hyperopt estimated optimum {}\".format(best_lstm))\n",
    "\n",
    "# Extract the best hyperparameters for LSTM\n",
    "best_units = int(best_lstm['units'])\n",
    "best_epochs = int(best_lstm['epochs'])\n",
    "\n",
    "# Create and train LSTM model with best hyperparameters\n",
    "lstm_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(units=best_units, input_shape=(X_train.shape[1], 1)),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "lstm_model.fit(X_train, y_train, epochs=best_epochs, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate LSTM model with best hyperparameters\n",
    "lstm_score = lstm_model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Accuracy of LSTM with best hyperparameters: ' + str(lstm_score[1]))\n",
    "\n",
    "# Predict using the LSTM model with best hyperparameters\n",
    "y_predict_lstm = (lstm_model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Calculate precision, recall, and F1-score for LSTM model\n",
    "precision_lstm, recall_lstm, fscore_lstm, _ = precision_recall_fscore_support(y_test, y_predict_lstm, average='weighted')\n",
    "print('Precision of LSTM with best hyperparameters: ' + str(precision_lstm))\n",
    "print('Recall of LSTM with best hyperparameters: ' + str(recall_lstm))\n",
    "print('F1-score of LSTM with best hyperparameters: ' + str(fscore_lstm))\n",
    "\n",
    "# Print classification report for LSTM model\n",
    "print(classification_report(y_test, y_predict_lstm))\n",
    "\n",
    "# Confusion matrix for LSTM model\n",
    "cm_lstm = confusion_matrix(y_test, y_predict_lstm)\n",
    "f, ax = plt.subplots(figsize=(5, 5))\n",
    "sns.heatmap(cm_lstm, annot=True, linewidths=0.5, linecolor=\"red\", fmt=\".0f\", ax=ax)\n",
    "plt.xlabel(\"y_pred\")\n",
    "plt.ylabel(\"y_true\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d354f5c4-5540-4a80-a956-8d958cbdc640",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_train=lstm_model.predict(X_train)\n",
    "lstm_test=lstm_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2bdace-cde2-498d-954b-429ccc7c9e16",
   "metadata": {},
   "source": [
    "#### LSTM without FCBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb1fef7-b72f-4725-a49e-6f86ecedd93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_lstm = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(64, input_shape=(X_train.shape[1], 1)),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train LSTM model\n",
    "model_lstm.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate LSTM model\n",
    "lstm_score = model_lstm.evaluate(X_test, y_test, verbose=0)\n",
    "print('Accuracy of LSTM: ' + str(lstm_score[1]))\n",
    "\n",
    "# Predict using the model\n",
    "y_predict_lstm = (model_lstm.predict(X_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision_lstm, recall_lstm, fscore_lstm, _ = precision_recall_fscore_support(y_test, y_predict_lstm, average='weighted')\n",
    "print('Precision of LSTM: ' + str(precision_lstm))\n",
    "print('Recall of LSTM: ' + str(recall_lstm))\n",
    "print('F1-score of LSTM: ' + str(fscore_lstm))\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_predict_lstm))\n",
    "\n",
    "# Confusion matrix\n",
    "cm_lstm = confusion_matrix(y_test, y_predict_lstm)\n",
    "f, ax = plt.subplots(figsize=(5, 5))\n",
    "sns.heatmap(cm_lstm, annot=True, linewidths=0.5, linecolor=\"red\", fmt=\".0f\", ax=ax)\n",
    "plt.xlabel(\"y_pred\")\n",
    "plt.ylabel(\"y_true\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191084e7-f9a5-4fcd-b687-c2d4b5a04217",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3399274-5bc1-4877-addf-cd274a2c8798",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44575baa-e2fd-48e5-b875-b0cb916cce70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ace10ad-d02e-45b2-bd6f-5e9a5864937e",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0935be1-25ea-44b4-a7cc-d38e85555594",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(random_state=0)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate SVM classifier\n",
    "svm_score = svm.score(X_test, y_test)\n",
    "print('Accuracy of SVM: ' + str(svm_score))\n",
    "\n",
    "# Predict using SVM classifier\n",
    "y_predict = svm.predict(X_test)\n",
    "y_true = y_test\n",
    "\n",
    "# Calculate precision, recall, and F1-score for SVM classifier\n",
    "precision, recall, fscore, _ = precision_recall_fscore_support(y_true, y_predict, average='weighted')\n",
    "print('Precision of SVM: ' + str(precision))\n",
    "print('Recall of SVM: ' + str(recall))\n",
    "print('F1-score of SVM: ' + str(fscore))\n",
    "\n",
    "# Print classification report for SVM classifier\n",
    "print(classification_report(y_true, y_predict))\n",
    "\n",
    "# Confusion matrix for SVM classifier\n",
    "cm = confusion_matrix(y_true, y_predict)\n",
    "f, ax = plt.subplots(figsize=(5, 5))\n",
    "sns.heatmap(cm, annot=True, linewidths=0.5, linecolor=\"red\", fmt=\".0f\", ax=ax)\n",
    "plt.xlabel(\"y_pred\")\n",
    "plt.ylabel(\"y_true\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8c03ae-df5d-41f9-a1cf-0a93f7272f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function for SVM\n",
    "def svm_objective(params):\n",
    "    params = {\n",
    "        'C': float(params['C']),\n",
    "        'kernel': str(params['kernel']),\n",
    "        'gamma': str(params['gamma']) if params['kernel'] == 'rbf' else 'scale',  # gamma only for 'rbf' kernel\n",
    "        'degree': int(params['degree']) if params['kernel'] == 'poly' else 3,  # degree only for 'poly' kernel\n",
    "    }\n",
    "    clf = SVC(**params, random_state=0)\n",
    "    clf.fit(X_train, y_train)\n",
    "    score = clf.score(X_test, y_test)\n",
    "    return {'loss': -score, 'status': STATUS_OK}\n",
    "\n",
    "# Define the hyperparameter configuration space for SVM\n",
    "svm_space = {\n",
    "    'C': hp.loguniform('C', np.log(1e-6), np.log(1e+3)),\n",
    "    'kernel': hp.choice('kernel', ['linear', 'poly', 'rbf', 'sigmoid']),\n",
    "    'gamma': hp.choice('gamma', ['scale', 'auto']),\n",
    "    'degree': hp.quniform('degree', 2, 5, 1)\n",
    "}\n",
    "\n",
    "# Perform hyperparameter optimization for SVM\n",
    "svm_trials = Trials()\n",
    "best_svm = fmin(fn=svm_objective, space=svm_space, algo=tpe.suggest, max_evals=50, trials=svm_trials)\n",
    "\n",
    "print(\"SVM: Hyperopt estimated optimum {}\".format(best_svm))\n",
    "\n",
    "# Extract the best hyperparameters for SVM\n",
    "best_svm_params = {\n",
    "    'C': best_svm['C'],\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'][best_svm['kernel']],\n",
    "    'gamma': ['scale', 'auto'][best_svm['gamma']],\n",
    "    'degree': int(best_svm['degree']) if 'degree' in best_svm else 3\n",
    "}\n",
    "\n",
    "# Create and train SVM model with best hyperparameters\n",
    "svm_hpo = SVC(**best_svm_params, random_state=0)\n",
    "svm_hpo.fit(X_train, y_train)\n",
    "svm_score = svm_hpo.score(X_test, y_test)\n",
    "print('Accuracy of SVM with best hyperparameters: ' + str(svm_score))\n",
    "\n",
    "# Predict using the SVM model with best hyperparameters\n",
    "y_predict_svm = svm_hpo.predict(X_test)\n",
    "y_true = y_test\n",
    "\n",
    "# Calculate precision, recall, and F1-score for SVM model\n",
    "precision_svm, recall_svm, fscore_svm, _ = precision_recall_fscore_support(y_true, y_predict_svm, average='weighted')\n",
    "print('Precision of SVM with best hyperparameters: ' + str(precision_svm))\n",
    "print('Recall of SVM with best hyperparameters: ' + str(recall_svm))\n",
    "print('F1-score of SVM with best hyperparameters: ' + str(fscore_svm))\n",
    "\n",
    "# Print classification report for SVM model\n",
    "print(classification_report(y_true, y_predict_svm))\n",
    "\n",
    "# Confusion matrix for SVM model\n",
    "cm_svm = confusion_matrix(y_true, y_predict_svm)\n",
    "f, ax = plt.subplots(figsize=(5, 5))\n",
    "sns.heatmap(cm_svm, annot=True, linewidths=0.5, linecolor=\"red\", fmt=\".0f\", ax=ax)\n",
    "plt.xlabel(\"y_pred\")\n",
    "plt.ylabel(\"y_true\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f0beaa-502f-4151-a508-ebc45dc2cb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_train=svm_hpo.predict(X_train)\n",
    "svm_test=svm_hpo.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab7aa65-abd4-4d48-8d67-353d194f76c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed0f9cab-7d10-44f5-924e-70016f738375",
   "metadata": {},
   "source": [
    "### Apply Stacking\n",
    "The ensemble model that combines the four ML models (DT, RF, CNN, LSTM, XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c448b759-3a6f-4874-9f2c-641d08af3b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_predictions_train = pd.DataFrame( {\n",
    "    'DecisionTree': dt_train.ravel(),\n",
    "        'RandomForest': rf_train.ravel(),\n",
    "     'CNN': cnn_train.ravel(),\n",
    "    'LSTM': lstm_train.ravel(),\n",
    "    'SVM':svm_train.ravel(),\n",
    "     'XgBoost': xg_train.ravel(),\n",
    "    })\n",
    "base_predictions_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb4c5c5-0a7c-4c16-8861-39f0339d4b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_train=dt_train.reshape(-1, 1)\n",
    "cnn_train=cnn_train.reshape(-1, 1)\n",
    "lstm_train=lstm_train.reshape(-1, 1)\n",
    "rf_train=rf_train.reshape(-1, 1)\n",
    "xg_train=xg_train.reshape(-1, 1)\n",
    "svm_train=svm_train.reshape(-1,1)\n",
    "\n",
    "dt_test=dt_test.reshape(-1, 1)\n",
    "cnn_test=cnn_test.reshape(-1, 1)\n",
    "lstm_test=lstm_test.reshape(-1, 1)\n",
    "rf_test=rf_test.reshape(-1, 1)\n",
    "xg_test=xg_test.reshape(-1, 1)\n",
    "svm_test=svm_test.reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e959015-b8b0-44c9-bcff-617eff4abb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d63f11a-92a7-439b-b0b0-eda83c6d1838",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.concatenate(( dt_train, cnn_train, lstm_train, rf_train, xg_train, svm_train), axis=1)\n",
    "x_test = np.concatenate(( dt_test, cnn_test, lstm_test, rf_test, xg_test, svm_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b4cd30-d555-493d-a00d-1c0cb053c2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "stk = xgb.XGBClassifier().fit(x_train, y_train)\n",
    "y_predict=stk.predict(x_test)\n",
    "y_true=y_test\n",
    "stk_score=accuracy_score(y_true,y_predict)\n",
    "print('Accuracy of Stacking: '+ str(stk_score))\n",
    "precision,recall,fscore,none= precision_recall_fscore_support(y_true, y_predict, average='weighted') \n",
    "print('Precision of Stacking: '+(str(precision)))\n",
    "print('Recall of Stacking: '+(str(recall)))\n",
    "print('F1-score of Stacking: '+(str(fscore)))\n",
    "print(classification_report(y_true,y_predict))\n",
    "cm=confusion_matrix(y_true,y_predict)\n",
    "f,ax=plt.subplots(figsize=(5,5))\n",
    "sns.heatmap(cm,annot=True,linewidth=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\n",
    "plt.xlabel(\"y_pred\")\n",
    "plt.ylabel(\"y_true\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb753400-f883-4975-81a9-b498704c6071",
   "metadata": {},
   "source": [
    "#### Hyperparameter optimization (HPO) of the stacking ensemble model (XGBoost) using Bayesian optimization with tree-based Parzen estimator (BO-TPE)\n",
    "Based on the GitHub repo for HPO: https://github.com/LiYangHart/Hyperparameter-Optimization-of-Machine-Learning-Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef582a2f-b364-4efc-a1c8-d944900d849a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    params = {\n",
    "        'n_estimators': int(params['n_estimators']), \n",
    "        'max_depth': int(params['max_depth']),\n",
    "        'learning_rate':  abs(float(params['learning_rate'])),\n",
    "\n",
    "    }\n",
    "    clf = xgb.XGBClassifier( **params)\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    score = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    return {'loss':-score, 'status': STATUS_OK }\n",
    "\n",
    "space = {\n",
    "    'n_estimators': hp.quniform('n_estimators', 10, 100, 5),\n",
    "    'max_depth': hp.quniform('max_depth', 4, 100, 1),\n",
    "    'learning_rate': hp.normal('learning_rate', 0.01, 0.9),\n",
    "}\n",
    "\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=20)\n",
    "print(\"XGBoost: Hyperopt estimated optimum {}\".format(best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f022a47b-5bf5-4c31-8689-c556c1de879b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xg = xgb.XGBClassifier(learning_rate= 0.22000784172473817, n_estimators = 35, max_depth = 49)\n",
    "xg.fit(x_train,y_train)\n",
    "xg_score=xg.score(x_test,y_test)\n",
    "y_predict=xg.predict(x_test)\n",
    "y_true=y_test\n",
    "print('Accuracy of XGBoost: '+ str(xg_score))\n",
    "precision,recall,fscore,none= precision_recall_fscore_support(y_true, y_predict, average='weighted') \n",
    "print('Precision of XGBoost: '+(str(precision)))\n",
    "print('Recall of XGBoost: '+(str(recall)))\n",
    "print('F1-score of XGBoost: '+(str(fscore)))\n",
    "print(classification_report(y_true,y_predict))\n",
    "cm=confusion_matrix(y_true,y_predict)\n",
    "f,ax=plt.subplots(figsize=(5,5))\n",
    "sns.heatmap(cm,annot=True,linewidth=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\n",
    "plt.xlabel(\"y_pred\")\n",
    "plt.ylabel(\"y_true\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1be04f-57be-4381-8c74-25b44e61fd51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d50261d-adbd-4611-b044-f8c4370bf5e6",
   "metadata": {},
   "source": [
    "## Anomaly Based Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a584589c-5206-466a-a577-3f607770f0f9",
   "metadata": {},
   "source": [
    "### Generate the port-scan datasets for unknown attack detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c25fdb-fb15-4c7c-8284-0285144f2f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.read_csv('./data/CICIDS2017_sample_km.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c7c80a-1a56-4ebb-a568-a76edc6fe68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.Label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0e8c3f-c57c-43f7-8ea7-8eef781f8a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2[df2['Label'] != 5]\n",
    "df3['Label'][df3['Label'] > 0] = 1\n",
    "df3.to_csv('./data/CICIDS2017_sample_km_without_portscan.csv',index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45490f0-b88c-4473-bd2b-257aaf99b40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df2[df2['Label'] == 5]\n",
    "df4['Label'][df4['Label'] == 5] = 1\n",
    "df4.to_csv('./data/CICIDS2017_sample_km_portscan.csv',index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5301b1-8cea-43de-965c-99ebbd8fa2cf",
   "metadata": {},
   "source": [
    "### Read the generated datasets for unknown attack detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803b9d7a-9a57-4533-81ce-32b3661e8e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv('./data/CICIDS2017_sample_km_without_portscan.csv')\n",
    "df4 = pd.read_csv('./data/CICIDS2017_sample_km_portscan.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f301e33a-5cc1-43cd-875f-1d06e864636a",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df3.drop(['Label'],axis=1).dtypes[df3.dtypes != 'object'].index\n",
    "df3[features] = df3[features].apply(\n",
    "    lambda x: (x - x.mean()) / (x.std()))\n",
    "df4[features] = df4[features].apply(\n",
    "    lambda x: (x - x.mean()) / (x.std()))\n",
    "df3 = df3.fillna(0)\n",
    "df4 = df4.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed103c48-7a2b-4110-a71d-8a00d0fc4e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.Label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84d1f7b-269d-4957-ad66-59a5f176df43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.Label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0900cf5-e8a8-4453-b11f-88cf8a0b2dbb",
   "metadata": {},
   "source": [
    "##### Aborting Anomaly Based here due to low sample size for sampled dataset!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd044f5-2bfe-4fe9-824e-cebe0ebd1dd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
